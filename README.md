# :world_map: The Roadmap for LLMs

![image-20230727100138263](Roadmap%20of%20LLMs.assets/image-20230727100138263.png)

> ğŸš§ æŒç»­æ›´æ–°...  [çŸ¥ä¹ï¼Œæ¬¢è¿äº¤æµ](https://www.zhihu.com/people/HiFuture_ToMyDream)   [å…¨éƒ¨å¤§æ¨¡å‹åˆ—è¡¨](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit?pli=1#gid=1158069878)
>
> Goal: åŠæ—¶æ›´æ–°æœ€æ–°çš„æ¨¡å‹ï¼Œè¯¦ç»†è§£è¯»æŠ€æœ¯ç»†èŠ‚

[:world_map: The Roadmap for LLMs](#-world-map--the-roadmap-for-llms)

[ä¸€ã€Foundation model (åŸºåº§æ¨¡å‹)](#--foundation-model-------)

* [1.1 Google ç³»](#11-google--)
* [1.2  Metaç³»](#12--meta-)
* [1.3 OpenAIç³»](#13-openai-)
* [1.4 EleutherAI](#14-eleutherai)
* [1.5 å…¶ä»–ç§‘æŠ€å…¬å¸å’Œç ”ç©¶é™¢](#15-----------)
* [:key: è®­ç»ƒæ¡†æ¶å’Œæ¨¡å‹è®­ç»ƒTricks](#-key-----------tricks)

[äºŒã€Instruction-tuning model (æŒ‡ä»¤å¾®è°ƒæ¨¡å‹)](#--instruction-tuning-model---------)
* [2.1 Googleç³»](#21-google-)
* [2.2 Metaç³»](#22-meta-)
* [2.3 OpenAIç³»](#23-openai-)
* [2.4 EleutherAI](#24-eleutherai)
* [2.5 å…¶ä»–ç§‘æŠ€å…¬å¸å’Œç ”ç©¶é™¢](#25-----------)
* [:key: è®­ç»ƒæ¡†æ¶å’Œæ¨¡å‹è®­ç»ƒTricks](#-key-----------tricks-1)

[ä¸‰ã€ Multimodal (å¤šæ¨¡æ€æ¨¡å‹)](#---multimodal--------)

# ä¸€ã€Foundation model (åŸºåº§æ¨¡å‹) 

## 1.1 Googleç³»

Google Brain (å·²åˆå¹¶åˆ°Google DeepMindéƒ¨é—¨)

|         æ¨¡å‹åç§°          |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡ |                   Paper                   |                             Code                             | Introduction                                                 |
| :-----------------------: | :-----: | :------: | :------: | :---------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------- |
|            T5             | 2019-10 |    æ˜¯    |   13B    | [Arxiv](https://arxiv.org/abs/1910.10683) | [github](https://github.com/google-research/text-to-text-transfer-transformer/) | [ T5 ä»‹ç»](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) |
|   T5X<br /> (æ¡†æ¶æ”¹è¿›)    | 2022-03 |    -     |    -     | [Arxiv](https://arxiv.org/abs/2203.17189) |       [github](https://github.com/google-research/t5x)       | [Youtube](https://www.youtube.com/watch?v=lHLX81qLk_8)       |
| LaMDA <br />(ChatBot LLM) | 2021-05 |    å¦    |   137B   |                     -                     |                              -                               | [LaMDAä»‹ç»](https://blog.google/technology/ai/lamda/)        |
|           PaLM            | 2022-04 |    å¦    |   540B   | [Arxiv](https://arxiv.org/abs/2204.02311) |                              -                               | [PaLMä»‹ç»](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) |

DeepMind (å·²åˆå¹¶åˆ°Google DeepMindéƒ¨é—¨)

|     æ¨¡å‹åç§°     |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡ |                     Paper                     | Code | Introduction                                              |
| :--------------: | :-----: | :------: | :------: | :-------------------------------------------: | :--: | --------------------------------------------------------- |
|   Gopher(åœ°é¼ )   | 2021-12 |    å¦    |   280B   | [Arxiv](https://arxiv.org/pdf/2112.11446.pdf) |  -   | -                                                         |
| Chinchilla(é¾™çŒ«) | 2022-04 |    å¦    |   70B    | [Arxiv](https://arxiv.org/pdf/2203.15556.pdf) |  -   | [Gopherä»‹ç»](https://www.youtube.com/watch?v=lHLX81qLk_8) |

Google DeepMind (23å¹´4æœˆåˆå¹¶Google Brainå’ŒDeepMindï¼Œå‘½åä¸ºGoogle DeepMind)

| æ¨¡å‹åç§° |  æ—¶é—´   | æ˜¯å¦å¼€æº |        å‚æ•°è§„æ¨¡         | Paper | Code | Introduction                                   |
| :------: | :-----: | :------: | :---------------------: | :---: | :--: | ---------------------------------------------- |
|  PaLM 2  | 2023-05 |    å¦    | 340B(å°é“æ¶ˆæ¯ï¼Œæœªè¯å®~) |   -   |  -   | [PaLM2ä»‹ç»](https://ai.google/discover/palm2/) |

**Latest:** æ›´å¼ºå¤§çš„æ¨¡å‹Geminiæ­£åœ¨è®­ç»ƒä¸­, [Ref](https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/)



## 1.2  Metaç³»

| æ¨¡å‹åç§° |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡  |                            Paper                             |                         Code                          |
| :------: | :-----: | :------: | :-------: | :----------------------------------------------------------: | :---------------------------------------------------: |
|   OPT    | 2022-05 |    æ˜¯    | 125M-175B |        [Arxiv](https://arxiv.org/pdf/2205.01068.pdf)         | [github](https://github.com/facebookresearch/metaseq) |
|  LLaMA   | 2023-02 |    æ˜¯    |  7B-65B   |          [Arxiv](https://arxiv.org/abs/2302.13971)           |  [github](https://github.com/facebookresearch/llama)  |
| LLaMA 2  | 2023-07 |    æ˜¯    |  7B-70B   | [Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) |  [github](https://github.com/facebookresearch/llama)  |



## 1.3 OpenAIç³»

![img](Roadmap%20of%20LLMs.assets/upload_af98c3d58bf03eef17312095483a78c8.png)

|           æ¨¡å‹åç§°            |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡  |                            Paper                             |                             Code                             |
| :---------------------------: | :-----: | :------: | :-------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|              GPT              | 2018-06 |    æ˜¯    |   117M    | [Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | [Hugging Face](https://huggingface.co/docs/transformers/model_doc/openai-gpt) |
|             GPT-2             | 2019-02 |    æ˜¯    | 150M-1.5B | [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | [Hugging Face](https://huggingface.co/docs/transformers/model_doc/gpt2) |
|             GPT-3             | 2020-05 |    å¦    | 125M-175B | [Wiki](https://en.wikipedia.org/wiki/GPT-3) [Arxiv](https://arxiv.org/abs/2005.14165) |
| GPT-3.5<br />(InstructionGPT) | 2022-01 |    å¦    |   175B    |  [Blog](https://openai.com/research/instruction-following)   |                              -                               |
|             GPT-4             | 2023-03 |    å¦    |   æœªçŸ¥    | [Blog](https://openai.com/research/instruction-following) <br />[GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) |                              -                               |



## 1.4 EleutherAI 

> ![img](Roadmap%20of%20LLMs.assets/120px-EleutherAI_logo.png) https://www.eleuther.ai/

|              æ¨¡å‹åç§°               |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡ |                   Paper                    |                            Code                            |
| :---------------------------------: | :-----: | :------: | :------: | :----------------------------------------: | :--------------------------------------------------------: |
| GPT-Neo <br />(GPT-2 architecture ) | 2021-03 |    æ˜¯    |   2.7B   | [Paper](https://zenodo.org/record/5297715) |      [github](https://github.com/EleutherAI/gpt-neo)       |
|                GPT-J                | 2021-06 |    æ˜¯    |    6B    | [Paper](https://arxiv.org/abs/2101.00027)  | [Hugging Face](https://huggingface.co/EleutherAI/gpt-j-6b) |
|              GPT-NeoX               | 2022-04 |    æ˜¯    |   20B    | [Paper](https://arxiv.org/abs/2204.06745)  |      [github](https://github.com/EleutherAI/gpt-neox)      |



## 1.5 å…¶ä»–ç§‘æŠ€å…¬å¸å’Œç ”ç©¶é™¢

|          æœºæ„          |      æ¨¡å‹åç§°      |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡ | Paper | Code |
| :--------------------: | :----------------: | :-----: | :------: | :------: | :---: | :--: |
|       Anthropic        | Anthropic-LM v4-s3 | 2021-12 |    å¦    |   52B    |   -   |  -   |
| åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ |     å¤©é¹°Aquila     | 2023-06 |    æ˜¯    |  7B/33B  |   -   |  -   |





## :key: è®­ç»ƒæ¡†æ¶å’Œæ¨¡å‹è®­ç»ƒTricks

ğŸš§ ...





# äºŒã€Instruction-tuning model (æŒ‡ä»¤å¾®è°ƒæ¨¡å‹)

## 2.1 Googleç³»

| æœºæ„                                                         |                 æ¨¡å‹åç§°                  | åŸºåº§æ¨¡å‹                  | æ˜¯å¦å¼€æº | Blog & Code                                                  | Paper                                         |
| ------------------------------------------------------------ | :---------------------------------------: | ------------------------- | -------- | ------------------------------------------------------------ | --------------------------------------------- |
| <img src="README.assets/1634806038075-5df7e9e5da6d0311fd3d53f9.png" width="30px"> BigScience |                    T0                     | T5                        | æ˜¯       | [Hugging Face](https://huggingface.co/bigscience/T0)         | [Arxiv](https://arxiv.org/pdf/2110.08207.pdf) |
| Google                                                       |                   FLAN                    | T5                        | æ˜¯       | [Hugging Face](https://huggingface.co/docs/transformers/model_doc/flan-t5) | [Arxiv](https://arxiv.org/abs/2210.11416)     |
| Google                                                       |             Flan-T5/Faln-PaLM             | T5/PaLM                   | å¦       | [github](https://github.com/google-research/FLAN#flan-2021-citation) | [Arxiv](https://arxiv.org/abs/2109.01652)     |
| DeepMind                                                     | **Sparrow<br />(ç”Ÿæˆäººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äºº)** | Chinchilla                | å¦       | [blog](https://www.deepmind.com/blog/building-safer-dialogue-agents) | -                                             |
| Google DeepMind                                              |  **Bard<br />(ç”Ÿæˆäººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äºº)**   | ä¹‹å‰æ˜¯LaMDAï¼Œåé¢æ˜¯PaLM 2 | å¦       | [Wiki](https://www.deepmind.com/blog/building-safer-dialogue-agents) | -                                             |



## 2.2 Metaç³»

| æœºæ„     |                           æ¨¡å‹åç§°                           | åŸºåº§æ¨¡å‹     | æ˜¯å¦å¼€æº | Blog & Code                                                  | Paper                                     |
| :------- | :----------------------------------------------------------: | ------------ | -------- | ------------------------------------------------------------ | ----------------------------------------- |
| Meta     |                           OPT-IML                            | OPT-175B     | æ˜¯       | [Hugging Face](https://huggingface.co/facebook/opt-iml-30b)  | [Arxiv](https://arxiv.org/abs/2212.12017) |
| Stanford | Alphaca (Alphace 7B)<br /><img src="README.assets/logo.png" width="250px"> | LLaMA-7B     | æ˜¯       | [Blog](https://crfm.stanford.edu/2023/03/13/alpaca.html)<br />[github](https://github.com/tatsu-lab/stanford_alpaca) | -                                         |
| Stanford | Vicuna (7B, 13B)<br /><img src="README.assets/vicuna-16904411351969.jpeg" width="120px"> | LLaMA-7B/13B | æ˜¯       | [Blog](https://lmsys.org/blog/2023-03-30-vicuna/)<br /><br />[github](https://github.com/lm-sys/FastChat#vicuna-weights) | -                                         |



## 2.3 OpenAIç³»

![image-20230727120740221](Roadmap%20of%20LLMs.assets/image-20230727120740221.png)

> Picture Ref: https://s10251.pcdn.co/wp-content/uploads/2023/03/2023-Alan-D-Thompson-GPT3-Family-Rev-1.png





## 2.4 EleutherAI 

| æ¨¡å‹åç§°     | åŸºåº§æ¨¡å‹ | æ˜¯å¦å¼€æº |                   Blog & Code                    |                   Paper                   |
| :----------- | :------- | :------- | :----------------------------------------------: | :---------------------------------------: |
| GPT-NeoX-20B | GPT-Neo  | æ˜¯       | [github](https://github.com/EleutherAI/gpt-neox) | [Arxiv](https://arxiv.org/abs/2204.06745) |



## 2.5 å…¶ä»–ç§‘æŠ€å…¬å¸å’Œç ”ç©¶é™¢

| æœºæ„                   |                           æ¨¡å‹åç§°                           |      åŸºåº§æ¨¡å‹      | æ˜¯å¦å¼€æº | Blog & Code                                                  | Paper                                     |
| :--------------------- | :----------------------------------------------------------: | :----------------: | :------- | ------------------------------------------------------------ | ----------------------------------------- |
| åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ | AquilaChat-7B<br />![Aquila_logo](README.assets/log-169044316851314.jpeg) |     Aquila-7B      | æ˜¯       | [Blog](https://model.baai.ac.cn/model-detail/100101)<br /><br />[Hugging Face](https://huggingface.co/BAAI/AquilaCode-multi) | -                                         |
| åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ | AquilaChat-33B<br />![Aquila_logo](README.assets/log-169044317529217.jpeg) |     Aquila-33B     | æ˜¯       | [Hugging Face](https://huggingface.co/BAAI/AquilaCode-multi) | -                                         |
| BigScience             |                            BLOOMZ                            |       BLOOM        | æ˜¯       | [Hugging Face](https://huggingface.co/bigscience/bloomz)     | [Arxiv](https://arxiv.org/abs/2211.01786) |
| Baidu                  |       æ–‡å¿ƒä¸€è¨€![logo](README.assets/logoErnieInfo.png)       |     ERNIE 3.0      | å¦       | [Website](https://yiyan.baidu.com/)                          | -                                         |
| Anthropic              |   Claude 2![å…‹åŠ³å¾· 2](README.assets/Claude2_Blog_V1-1.png)   | Anthropic-LM v4-s3 | å¦       | [Website](https://claude.ai/login)                           | -                                         |





## :key: è®­ç»ƒæ¡†æ¶å’Œæ¨¡å‹è®­ç»ƒTricks

ğŸš§ ...



# ä¸‰ã€ Multimodal (å¤šæ¨¡æ€æ¨¡å‹)

ğŸš§ ...







