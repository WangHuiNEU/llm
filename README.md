# :world_map: The Roadmap for LLMs

![image-20230727100138263](Roadmap%20of%20LLMs.assets/image-20230727100138263.png)

> ğŸš§ æŒç»­æ›´æ–°...  [çŸ¥ä¹ï¼Œæ¬¢è¿äº¤æµ](https://www.zhihu.com/people/HiFuture_ToMyDream)   [å…¨éƒ¨å¤§æ¨¡å‹åˆ—è¡¨](https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit?pli=1#gid=1158069878)
>
> Goal: åŠæ—¶æ›´æ–°æœ€æ–°çš„æ¨¡å‹ï¼Œè¯¦ç»†è§£è¯»æŠ€æœ¯ç»†èŠ‚

[:world_map: The Roadmap for LLMs](#-world-map--the-roadmap-for-llms)

[ä¸€ã€Foundation model (åŸºåº§æ¨¡å‹)](#--foundation-model-------)

* [1.1 Google ç³»](#11-google--)
* [1.2  Metaç³»](#12--meta-)
* [1.3 OpenAIç³»](#13-openai-)
* [1.4 EleutherAI](#14-eleutherai)
* [1.5 å…¶ä»–ç§‘æŠ€å…¬å¸å’Œç ”ç©¶é™¢](#15-----------)
* [:key: è®­ç»ƒæ¡†æ¶å’Œæ¨¡å‹è®­ç»ƒTricks](#-key-----------tricks)

[äºŒã€Instruction-tuning model (æŒ‡ä»¤å¾®è°ƒæ¨¡å‹)](#--instruction-tuning-model---------)
* [2.1 Googleç³»](#21-google-)
* [2.2 Metaç³»](#22-meta-)
* [2.3 OpenAIç³»](#23-openai-)
* [2.4 EleutherAI](#24-eleutherai)
* [2.5 å…¶ä»–ç§‘æŠ€å…¬å¸å’Œç ”ç©¶é™¢](#25-----------)
* [:key: è®­ç»ƒæ¡†æ¶å’Œæ¨¡å‹è®­ç»ƒTricks](#-key-----------tricks-1)

- [ä¸‰ã€ Multimodal (å¤šæ¨¡æ€æ¨¡å‹)](#---multimodal--------)

# ä¸€ã€Foundation model (åŸºåº§æ¨¡å‹) 

## 1.1 Google ç³»

Google Brain (å·²åˆå¹¶åˆ°Google DeepMindéƒ¨é—¨)

|         æ¨¡å‹åç§°          |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡ |                   Paper                   |                             Code                             | Introduction                                                 |
| :-----------------------: | :-----: | :------: | :------: | :---------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------- |
|            T5             | 2019-10 |    æ˜¯    |   13B    | [Arxiv](https://arxiv.org/abs/1910.10683) | [github](https://github.com/google-research/text-to-text-transfer-transformer/) | [ T5 ä»‹ç»](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) |
|   T5X<br /> (æ¡†æ¶æ”¹è¿›)    | 2022-03 |    -     |    -     | [Arxiv](https://arxiv.org/abs/2203.17189) |       [github](https://github.com/google-research/t5x)       | [Youtube](https://www.youtube.com/watch?v=lHLX81qLk_8)       |
| LaMDA <br />(ChatBot LLM) | 2021-05 |    å¦    |   137B   |                     -                     |                              -                               | [LaMDAä»‹ç»](https://blog.google/technology/ai/lamda/)        |
|           PaLM            | 2022-04 |    å¦    |   540B   | [Arxiv](https://arxiv.org/abs/2204.02311) |                              -                               | [PaLMä»‹ç»](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) |

DeepMind (å·²åˆå¹¶åˆ°Google DeepMindéƒ¨é—¨)

|     æ¨¡å‹åç§°     |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡ |                     Paper                     | Code | Introduction                                              |
| :--------------: | :-----: | :------: | :------: | :-------------------------------------------: | :--: | --------------------------------------------------------- |
|   Gopher(åœ°é¼ )   | 2021-12 |    å¦    |   280B   | [Arxiv](https://arxiv.org/pdf/2112.11446.pdf) |  -   | -                                                         |
| Chinchilla(é¾™çŒ«) | 2022-04 |    å¦    |   70B    | [Arxiv](https://arxiv.org/pdf/2203.15556.pdf) |  -   | [Gopherä»‹ç»](https://www.youtube.com/watch?v=lHLX81qLk_8) |

Google DeepMind (23å¹´4æœˆåˆå¹¶Google Brainå’ŒDeepMindï¼Œå‘½åä¸ºGoogle DeepMind)

| æ¨¡å‹åç§° |  æ—¶é—´   | æ˜¯å¦å¼€æº |        å‚æ•°è§„æ¨¡         | Paper | Code | Introduction                                   |
| :------: | :-----: | :------: | :---------------------: | :---: | :--: | ---------------------------------------------- |
|  PaLM 2  | 2023-05 |    å¦    | 340B(å°é“æ¶ˆæ¯ï¼Œæœªè¯å®~) |   -   |  -   | [PaLM2ä»‹ç»](https://ai.google/discover/palm2/) |

**Latest:** æ›´å¼ºå¤§çš„æ¨¡å‹Geminiæ­£åœ¨è®­ç»ƒä¸­, [Ref](https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/)



## 1.2  Metaç³»

| æ¨¡å‹åç§° |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡  |                            Paper                             |                         Code                          |
| :------: | :-----: | :------: | :-------: | :----------------------------------------------------------: | :---------------------------------------------------: |
|   OPT    | 2022-05 |    æ˜¯    | 125M-175B |        [Arxiv](https://arxiv.org/pdf/2205.01068.pdf)         | [github](https://github.com/facebookresearch/metaseq) |
|  LLaMA   | 2023-02 |    æ˜¯    |  7B-65B   |          [Arxiv](https://arxiv.org/abs/2302.13971)           |  [github](https://github.com/facebookresearch/llama)  |
| LLaMA 2  | 2023-07 |    æ˜¯    |  7B-70B   | [Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) |  [github](https://github.com/facebookresearch/llama)  |



## 1.3 OpenAIç³»

![img](Roadmap%20of%20LLMs.assets/upload_af98c3d58bf03eef17312095483a78c8.png)

|           æ¨¡å‹åç§°            |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡  |                            Paper                             |                             Code                             |
| :---------------------------: | :-----: | :------: | :-------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|              GPT              | 2018-06 |    æ˜¯    |   117M    | [Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | [Hugging Face](https://huggingface.co/docs/transformers/model_doc/openai-gpt) |
|             GPT-2             | 2019-02 |    æ˜¯    | 150M-1.5B | [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | [Hugging Face](https://huggingface.co/docs/transformers/model_doc/gpt2) |
|             GPT-3             | 2020-05 |    å¦    | 125M-175B | [Wiki](https://en.wikipedia.org/wiki/GPT-3) [Arxiv](https://arxiv.org/abs/2005.14165) |
| GPT-3.5<br />(InstructionGPT) | 2022-01 |    å¦    |   175B    |  [Blog](https://openai.com/research/instruction-following)   |                              -                               |
|             GPT-4             | 2023-03 |    å¦    |   æœªçŸ¥    | [Blog](https://openai.com/research/instruction-following) <br />[GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) |                              -                               |



## 1.4 EleutherAI 

> ![img](Roadmap%20of%20LLMs.assets/120px-EleutherAI_logo.png) https://www.eleuther.ai/

|              æ¨¡å‹åç§°               |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡ |                   Paper                    |                            Code                            |
| :---------------------------------: | :-----: | :------: | :------: | :----------------------------------------: | :--------------------------------------------------------: |
| GPT-Neo <br />(GPT-2 architecture ) | 2021-03 |    æ˜¯    |   2.7B   | [Paper](https://zenodo.org/record/5297715) |      [github](https://github.com/EleutherAI/gpt-neo)       |
|                GPT-J                | 2021-06 |    æ˜¯    |    6B    | [Paper](https://arxiv.org/abs/2101.00027)  | [Hugging Face](https://huggingface.co/EleutherAI/gpt-j-6b) |
|              GPT-NeoX               | 2022-04 |    æ˜¯    |   20B    | [Paper](https://arxiv.org/abs/2204.06745)  |      [github](https://github.com/EleutherAI/gpt-neox)      |



## 1.5 å…¶ä»–ç§‘æŠ€å…¬å¸å’Œç ”ç©¶é™¢

|          æœºæ„          |      æ¨¡å‹åç§°      |  æ—¶é—´   | æ˜¯å¦å¼€æº | å‚æ•°è§„æ¨¡ | Paper | Code |
| :--------------------: | :----------------: | :-----: | :------: | :------: | :---: | :--: |
|       Anthropic        | Anthropic-LM v4-s3 | 2021-12 |    å¦    |   52B    |   -   |  -   |
| åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ |     å¤©é¹°Aquila     | 2023-06 |    æ˜¯    |  7B/33B  |   -   |  -   |





## :key: è®­ç»ƒæ¡†æ¶å’Œæ¨¡å‹è®­ç»ƒTricks

ğŸš§ ...





# äºŒã€Instruction-tuning model (æŒ‡ä»¤å¾®è°ƒæ¨¡å‹)

## 2.1 Googleç³»

| å•ä½         | æ¨¡å‹åç§°                         | åŸºåº§æ¨¡å‹                  | æ˜¯å¦å¼€æº |
| ------------ | -------------------------------- | ------------------------- | -------- |
| Hugging Face | T0                               | T5                        | æ˜¯       |
| Google       | FLAN                             | T5                        | å¦       |
| Google       | Flan-T5/Faln-PaLM                | T5/PaLM                   | å¦       |
| Google       | **Bard(ç”Ÿæˆäººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äºº)** | ä¹‹å‰æ˜¯LaMDAï¼Œåé¢æ˜¯PaLM 2 | å¦       |



## 2.2 Metaç³»

| å•ä½     | æ¨¡å‹åç§° | åŸºåº§æ¨¡å‹ | æ˜¯å¦å¼€æº |
| -------- | -------- | -------- | -------- |
| Meta     | OPT-IML  | OPT-175B | æ˜¯       |
| Stanford | Alphaca  | LLaMA    | æ˜¯       |
| Stanford | Vicuna   | LLaMA    | æ˜¯       |





## 2.3 OpenAIç³»

![image-20230727120740221](Roadmap%20of%20LLMs.assets/image-20230727120740221.png)

> Picture Ref: https://s10251.pcdn.co/wp-content/uploads/2023/03/2023-Alan-D-Thompson-GPT3-Family-Rev-1.png





## 2.4 EleutherAI 

| æ¨¡å‹åç§° | åŸºåº§æ¨¡å‹ | æ˜¯å¦å¼€æº |
| :------- | :------- | :------- |
| GPT-NeoX | GPT-Neo  | æ˜¯       |





## 2.5 å…¶ä»–ç§‘æŠ€å…¬å¸å’Œç ”ç©¶é™¢

| æœºæ„                   | æ¨¡å‹åç§°       | åŸºåº§æ¨¡å‹           | æ˜¯å¦å¼€æº |
| :--------------------- | :------------- | :----------------- | :------- |
| åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ | AquilaChat-7B  | Aquila-7B          | æ˜¯       |
| åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ | AquilaChat-33B | Aquila-33B         | æ˜¯       |
| BigScience             | BLOOMZ         | BLOOM              | æ˜¯       |
| Baidu                  | æ–‡å¿ƒä¸€è¨€       | ERNIE 3.0          | å¦       |
| Anthropic              | Claude 2       | Anthropic-LM v4-s3 | å¦       |



## :key: è®­ç»ƒæ¡†æ¶å’Œæ¨¡å‹è®­ç»ƒTricks

ğŸš§ ...



# ä¸‰ã€ Multimodal (å¤šæ¨¡æ€æ¨¡å‹)

ğŸš§ ...







